@article{Chen2017MV3D,
  abstract = {This paper aims at high-accuracy 3D object detection in autonomous driving scenario. We propose Multi-View 3D networks (MV3D), a sensory-fusion framework that takes both LIDAR point cloud and RGB images as input and predicts oriented 3D bounding boxes. We encode the sparse 3D point cloud with a compact multi-view representation. The network is composed of two subnetworks: one for 3D object proposal generation and another for multi-view feature fusion. The proposal network generates 3D candidate boxes efficiently from the bird's eye view representation of 3D point cloud. We design a deep fusion scheme to combine region-wise features from multiple views and enable interactions between intermediate layers of different paths. Experiments on the challenging KITTI benchmark show that our approach outperforms the state-of-the-art by around 25% and 30% AP on the tasks of 3D localization and 3D detection. In addition, for 2D detection, our approach obtains 10.3% higher AP than the state-of-the-art on the hard data among the LIDAR-based methods.},
  author = {Chen, Xiaozhi and Ma, Huimin and Wan, Ji and Li, Bo and Xia, Tian},
  doi = {10.1109/CVPR.2017.690},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {3D object detection, sensor fusion, autonomous driving},
  number = {1},
  publisher = {IEEE},
  volume = {2017},
  series = {CVPR},
  title = {Multi-View 3D Object Detection Network for Autonomous Driving},
  url = {https://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_Multi-View_3D_Object_CVPR_2017_paper.pdf},
  year = {2017}
}
@inproceedings{Qi2018Frustum,
  abstract = {In this work, we study 3D object detection from RGB-D data in both indoor and outdoor scenes. While previous methods focus on images or 3D voxels, often obscuring natural 3D patterns and invariances of 3D data, we directly operate on raw point clouds by popping up RGB-D scans. However, a key challenge of this approach is how to efficiently localize objects in point clouds of large-scale scenes (region proposal). Instead of solely relying on 3D proposals, our method leverages both mature 2D object detectors and advanced 3D deep learning for object localization, achieving efficiency as well as high recall for even small objects. Benefited from learning directly in raw point clouds, our method is also able to precisely estimate 3D bounding boxes even under strong occlusion or with very sparse points. Evaluated on KITTI and SUN RGB-D 3D detection benchmarks, our method outperforms the state of the art by remarkable margins while having real-time capability.},
  author = {Qi, Charles R. and Liu, Wei and Wu, Chenxia and Su, Hao and Guibas, Leonidas J.},
  doi = {10.1109/CVPR.2018.00959},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  keywords = {3D object detection, point cloud, RGB-D, deep learning},
  number = {1},
  publisher = {IEEE},
  volume = {2018},
  series = {CVPR},
  title = {Frustum PointNets for 3D Object Detection from RGB-D Data},
  url = {https://openaccess.thecvf.com/content_cvpr_2018/html/Qi_Frustum_PointNets_for_CVPR_2018_paper.html},
  year = {2018}
}

@article{Zhou2018VoxelNet,
  abstract = {Accurate detection of objects in 3D point clouds is a central problem in many applications, such as autonomous navigation, housekeeping robots, and augmented/virtual reality. To interface a highly sparse LiDAR point cloud with a region proposal network (RPN), most existing efforts have focused on hand-crafted feature representations, for example, a bird’s eye view projection. In this work, we remove the need of manual feature engineering for 3D point clouds and propose VoxelNet, a generic 3D detection network that unifies feature extraction and bounding box prediction into a single stage, end-to-end trainable deep network. Specifically, VoxelNet divides a point cloud into equally spaced 3D voxels and transforms a group of points within each voxel into a unified feature representation through the newly introduced voxel feature encoding (VFE) layer. In this way, the point cloud is encoded as a descriptive volumetric representation, which is then connected to a RPN to generate detections. Experiments on the KITTI car detection benchmark show that VoxelNet outperforms the state-of-the-art LiDAR based 3D detection methods by a large margin.},
  author = {Zhou, Yin and Tuzel, Oncel},
  doi = {10.1109/CVPR.2018.00117},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {voxelization, LiDAR, 3D detection, deep learning},
  number = {1},
  publisher = {IEEE},
  volume = {2018},
  series = {CVPR},
  title = {VoxelNet: End-to-End Learning for Point Cloud Based 3D Object Detection},
  url = {https://arxiv.org/abs/1711.06396},
  year = {2018}
}
@article{Lang2019PointPillars,
  abstract = {}|
  author = {Lang, Alex H. and Vora, Sourabh and Caesar, Holger and Zhou, Lubing and Yang, Jiong and Beijbom, Oscar},
  doi = {10.1109/CVPR.2019.00916},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {lidar, real-time detection, 3D object detection, point cloud},
  number = {1},
  publisher = {IEEE},
  volume = {2019},
  series = {CVPR},
  title = {PointPillars: Fast Encoders for Object Detection from Point Clouds},
  url = {https://arxiv.org/abs/1812.05784},
  year = {2019}
}
@article{Shi2019PointRCNN,
  abstract = {Object detection in point clouds is an important aspect of many robotics applications such as autonomous driving. In this paper, we consider the problem of encoding a point cloud into a format appropriate for a downstream detection pipeline. Recent literature suggests two types of encoders; fixed encoders tend to be fast but sacrifice accuracy, while encoders that are learned from data are more accurate, but slower. In this work, we propose PointPillars, a novel encoder which utilizes PointNets to learn a representation of point clouds organized in vertical columns (pillars). While the encoded features can be used with any standard 2D convolutional detection architecture, we further propose a lean downstream network. Extensive experimentation shows that PointPillars outperforms previous encoders with respect to both speed and accuracy by a large margin.},
  author = {Shi, Shaoshuai and Wang, Xiaogang and Li, Hongsheng},
  doi = {10.1109/CVPR.2019.00912},
  journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {3D detection, LiDAR, point cloud segmentation, proposal generation},
  number = {1},
  publisher = {IEEE},
  volume = {2019},
  series = {CVPR},
  title = {PointRCNN: 3D Object Proposal Generation and Detection from Point Cloud},
  url = {https://arxiv.org/abs/1812.04244},
  year = {2019}
}
@article{Shi2020PointGNN,
  abstract = {In this paper, we propose a graph neural network to detect objects from a LiDAR point cloud. Towards this end, we encode the point cloud efficiently in a fixed radius near-neighbors graph. We design a graph neural network, named Point-GNN, to predict the category and shape of the object that each vertex in the graph belongs to. In Point-GNN, we propose an auto-registration mechanism to reduce translation variance, and also design a box merging and scoring operation to combine detections from multiple vertices accurately. Our experiments on the KITTI benchmark show the proposed approach achieves leading accuracy using the point cloud alone and can even surpass fusion-based algorithms.},
  author = {Shi, Weijing and Rajkumar, Ragunathan},
  doi = {10.1109/CVPR42600.2020.00215},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {graph neural networks, 3D detection, point cloud, GNN},
  number = {1},
  publisher = {IEEE},
  volume = {2020},
  series = {CVPR},
  title = {Point-GNN: Graph Neural Network for 3D Object Detection in a Point Cloud},
  url = {https://arxiv.org/abs/2003.01251},
  year = {2020}
}
@article{Shi2020PVRCNN,
  abstract = {We present a novel and high-performance 3D object detection framework, named PointVoxel-RCNN (PV-RCNN), for accurate 3D object detection from point clouds. Our proposed method deeply integrates both 3D voxel Convolutional Neural Network (CNN) and PointNet-based set abstraction to learn more discriminative point cloud features. It takes advantages of efficient learning and high-quality proposals of the 3D voxel CNN and the flexible receptive fields of the PointNet-based networks. Experiments on both the KITTI dataset and the Waymo Open dataset show that our proposed PV-RCNN surpasses state-of-the-art 3D detection methods with remarkable margins.},
  author = {Shi, Shaoshuai and Guo, Chaoxu and Jiang, Li and Wang, Zhe and Shi, Jianping and Wang, Xiaogang and Li, Hongsheng},
  doi = {10.1109/CVPR42600.2020.00476},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {3D detection, point-voxel fusion, LiDAR, object detection},
  number = {1},
  publisher = {IEEE},
  volume = {2020},
  series = {CVPR},
  title = {PV-RCNN: Point-Voxel Feature Set Abstraction for 3D Object Detection},
  url = {https://arxiv.org/abs/1912.13192},
  year = {2020}
}
@article{Vora2020PointPainting,
  abstract = {Camera and lidar are important sensor modalities for robotics in general and self-driving cars in particular. The sensors provide complementary information offering an opportunity for tight sensor-fusion. Surprisingly, lidar-only methods outperform fusion methods on the main benchmark datasets, suggesting a gap in the literature. In this work, we propose PointPainting: a sequential fusion method to fill this gap. PointPainting works by projecting lidar points into the output of an image-only semantic segmentation network and appending the class scores to each point. The appended (painted) point cloud can then be fed to any lidar-only method. Experiments show large improvements on three different state-of-the art methods, Point-RCNN, VoxelNet and PointPillars on the KITTI and nuScenes datasets.},
  author = {Vora, Sourabh and Lang, Alex H. and Helou, Bassam and Beijbom, Oscar},
  doi = {10.1109/CVPR42600.2020.00538},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {fusion, semantic segmentation, lidar, camera},
  number = {1},
  publisher = {IEEE},
  volume = {2020},
  series = {CVPR},
  title = {PointPainting: Sequential Fusion for 3D Object Detection},
  url = {https://arxiv.org/abs/1911.10150},
  year = {2020}
}
@article{Wang2020PseudoLiDAR,
  abstract = {3D object detection is an essential task in autonomous driving. Recent techniques excel with highly accurate detection rates, provided the 3D input data is obtained from precise but expensive LiDAR technology. Approaches based on cheaper monocular or stereo imagery data have, until now, resulted in drastically lower accuracies}, — a gap that is commonly attributed to poor image-based depth estimation. However, in this paper we argue that it is not the quality of the data but its representation that accounts for the majority of the difference. Taking the inner workings of convolutional neural networks into consideration, we propose to convert image-based depth maps to pseudo-LiDAR representations — essentially mimicking the LiDAR signal. With this representation we can apply different existing LiDAR-based detection algorithms. On the popular KITTI benchmark, our approach achieves impressive improvements over the existing state-of-the-art in image-based performance.},
  author = {Wang, Yan and Chao, Wei-Lun and Garg, Divyansh and Hariharan, Bharath and Campbell, Mark and Weinberger, Kilian Q.},
  doi = {10.1109/CVPR42600.2020.00811},
  journal = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  keywords = {pseudo-LiDAR, stereo depth, monocular depth, 3D detection},
  number = {1},
  publisher = {IEEE},
  volume = {2020},
  series = {CVPR},
  title = {Pseudo-LiDAR from Visual Depth Estimation: Bridging the Gap in 3D Object Detection for Autonomous Driving},
  url = {https://arxiv.org/abs/1812.07179},
  year = {2020}
}
@article{Wang2021FCOS3D,
  abstract = {Monocular 3D object detection is an important task for autonomous driving considering its advantage of low cost. It is much more challenging than conventional 2D cases due to its inherent ill-posed property, which is mainly reflected in the lack of depth information. In this paper, we study this problem with a practice built on a fully convolutional single-stage detector and propose a general framework FCOS3D. Our solution achieves 1st place out of all the vision-only methods in the nuScenes 3D detection challenge of NeurIPS 2020.},|
  author = {Wang, Tai and Zhu, Xinge and Pang, Jiangmiao and Lin, Dahua},
  doi = {10.1109/ICCV48922.2021.00912},
  journal = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  keywords = {monocular 3D detection, anchor-free, autonomous driving},
  number = {1},
  publisher = {IEEE},
  volume = {2021},
  series = {ICCV},
  title = {FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection},
  url = {https://arxiv.org/abs/2104.10956},
  year = {2021}
}